{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:\n",
    "- Question + <'pad'> + context\n",
    "\n",
    "Output:\n",
    "- The start index and end index of the extracted information from context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = [\n",
    "    {\n",
    "        'context': 'My name is Thang and I am from Vietnam',\n",
    "        'question': 'What is my name?',\n",
    "        'answer': 'Thang'\n",
    "    },\n",
    "    {\n",
    "        'context': 'I love painting and my favorite artist is Vincent Van Gough',\n",
    "        'question': 'What is my favorite activity?',\n",
    "        'answer' : 'painting'\n",
    "    },\n",
    "    {\n",
    "        'context' : 'I am studying Computer Science at Nanyang Technological University',\n",
    "        'question' : 'What am I studying?',\n",
    "        'answer' : 'Computer Science'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'cats']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "text = 'I love cats'\n",
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<sep>': 4,\n",
       " 'vietnam<sep>what': 28,\n",
       " 'technological': 24,\n",
       " '<bos>': 2,\n",
       " '<unk>': 0,\n",
       " 'artist': 15,\n",
       " '<eos>': 3,\n",
       " '<pad>': 1,\n",
       " 'i': 5,\n",
       " 'is': 6,\n",
       " 'am': 9,\n",
       " 'my': 7,\n",
       " '?': 8,\n",
       " 'name': 12,\n",
       " 'and': 10,\n",
       " 'favorite': 11,\n",
       " 'studying': 13,\n",
       " 'activity': 14,\n",
       " 'at': 16,\n",
       " 'computer': 17,\n",
       " 'from': 18,\n",
       " 'gough<sep>what': 19,\n",
       " 'love': 20,\n",
       " 'nanyang': 21,\n",
       " 'painting': 22,\n",
       " 'science': 23,\n",
       " 'thang': 25,\n",
       " 'university<sep>what': 26,\n",
       " 'van': 27,\n",
       " 'vincent': 29}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def yield_tokens(data):\n",
    "    for item in data:\n",
    "        yield(tokenizer(item['context'] + '<sep>' + item['question']))\n",
    "\n",
    "#create vocab\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(qa_dataset),\n",
    "    specials=['<unk>', '<pad>', '<bos>', '<eos>', '<sep>']\n",
    ")\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 1\n",
    "def pad_and_truncate(input_ids, max_seq_len):\n",
    "    if len(input_ids) > max_seq_len:\n",
    "        input_ids = input_ids[:max_seq_len]\n",
    "    elif len(input_ids) < max_seq_len:\n",
    "        input_ids += [PAD_IDX] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 22\n",
    "def vectorize(question, context, answer, max_seq_len):\n",
    "    input_text = question + '<sep>' + context\n",
    "    input_ids = [vocab[token] for token in tokenizer(input_text)]\n",
    "    input_ids = pad_and_truncate(input_ids, MAX_SEQ_LENGTH)\n",
    "\n",
    "    answer_ids = [vocab[token] for token in tokenizer(answer)]\n",
    "    start_positions = input_ids.index(answer_ids[0])\n",
    "    end_positions = start_positions + len(answer_ids) - 1\n",
    "\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    start_positions = torch.tensor(start_positions, dtype=torch.long)\n",
    "    end_positions = torch.tensor(end_positions, dtype=torch.long)\n",
    "\n",
    "    return input_ids, start_positions, end_positions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(nn.Module):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        question_text = item['question']\n",
    "        context_text = item['context']\n",
    "        answer_text = item['answer']\n",
    "\n",
    "        input_ids, start_positions, end_positions = vectorize(\n",
    "            question_text, context_text, answer_text, 22\n",
    "        )\n",
    "\n",
    "        return input_ids, start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(input_ids):\n",
    "    return ' '.join([vocab.lookup_token(token) for token in input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  9,  5, 13,  8,  0,  9, 13, 17, 23, 16, 21, 24,  0,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1],\n",
      "        [ 0,  6,  7, 11, 14,  8,  0, 20, 22, 10,  7, 11, 15,  6, 29, 27,  0,  1,\n",
      "          1,  1,  1,  1]]) tensor([8, 8]) tensor([9, 8])\n",
      "tensor([[ 0,  6,  7, 12,  8,  0, 12,  6, 25, 10,  5,  9, 18,  0,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1]]) tensor([8]) tensor([8])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = QADataset(qa_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for batch in train_loader:\n",
    "    q, c, a = batch\n",
    "    print(q, c, a)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: bidirectional lstm\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, embedding_dim, hidden_size,\n",
    "                 n_layers):\n",
    "        super().__init__()\n",
    "        self.input_embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.start_linear = nn.Linear(hidden_size*2, 1)\n",
    "        self.end_linear = nn.Linear(hidden_size*2, 1)\n",
    "\n",
    "    def forward(self, text):\n",
    "        input_embedded = self.input_embedding(text)\n",
    "\n",
    "        lstm_out, _ = self.lstm(input_embedded)\n",
    "        start_logits = self.start_linear(lstm_out).squeeze(-1)\n",
    "        end_logits = self.end_linear(lstm_out).squeeze(-1)\n",
    "\n",
    "    \n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "#Model parameters\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_SIZE = 128\n",
    "VOCAB_SIZE = len(vocab)\n",
    "N_LAYERS = 2\n",
    "\n",
    "model = QAModel(\n",
    "    VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, N_LAYERS\n",
    ")\n",
    "\n",
    "input_context = torch.randint(0, 10, size=(1, 10))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    start_logits, end_logits = model(input_context)\n",
    "\n",
    "print(start_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11022062599658966\n",
      "0.4110000431537628\n",
      "0.13101322948932648\n",
      "0.6656321287155151\n",
      "0.12594982981681824\n",
      "0.21103407442569733\n",
      "0.01870870217680931\n",
      "0.49638795852661133\n",
      "0.1484886258840561\n",
      "0.019184071570634842\n",
      "0.019021131098270416\n",
      "0.4201688766479492\n",
      "0.15264801681041718\n",
      "0.02022932469844818\n",
      "0.022151129320263863\n",
      "0.13079124689102173\n",
      "0.08015856146812439\n",
      "0.028210598975419998\n",
      "0.0825396478176117\n",
      "0.027810359373688698\n",
      "0.07384565472602844\n",
      "0.024248000234365463\n",
      "0.05756865441799164\n",
      "0.023093003779649734\n",
      "0.046066176146268845\n",
      "0.015439847484230995\n",
      "0.03565260395407677\n",
      "0.01469854824244976\n",
      "0.012510443106293678\n",
      "0.04787231981754303\n",
      "0.02847272902727127\n",
      "0.009645873680710793\n",
      "0.026761554181575775\n",
      "0.008145080879330635\n",
      "0.008858866058290005\n",
      "0.039425767958164215\n",
      "0.021974265575408936\n",
      "0.00964362546801567\n",
      "0.007578282617032528\n",
      "0.031262706965208054\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "EPOCHS = 20\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=LR\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for _ in range(EPOCHS):\n",
    "    for idx, (input_ids, start_positions, end_positions) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        start_logits, end_logits = model(input_ids)\n",
    "        start_loss = criterion(start_logits, start_positions)\n",
    "        end_loss = criterion(end_logits, end_positions)\n",
    "        loss = start_loss + end_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love painting and my favorite artist is Vincent Van Gough\n",
      "What is my favorite activity?\n",
      "painting\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample = qa_dataset[1]\n",
    "    context, question, answer = sample.values()\n",
    "    input_ids, start_position, end_position = vectorize(context, question, answer, MAX_SEQ_LENGTH)\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    start_logits, end_logits = model(input_ids)\n",
    "\n",
    "    offset = len(tokenizer(question)) + 2\n",
    "    start_position = torch.argmax(start_logits, dim=1).numpy()[0]\n",
    "    end_position = torch.argmax(end_logits, dim=1).numpy()[0]\n",
    "\n",
    "    start_postion -= offset\n",
    "    end_position -= offset\n",
    "\n",
    "    start_position = max(start_position, 0)\n",
    "    end_position = min(end_position, len(tokenizer(context)) - 1)\n",
    "\n",
    "    if end_position >= start_postion:\n",
    "        #extract the predicted answer span\n",
    "        context_tokens = tokenizer(context)\n",
    "        predicted_answer_tokens = context_tokens[start_position:end_postion+1]\n",
    "        predicted_answer = ' '.join(predicted_answer_tokens)\n",
    "        print(context)\n",
    "        print(question)\n",
    "        print(predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devinthang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
